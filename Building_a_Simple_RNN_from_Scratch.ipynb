{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RNN"
      ],
      "metadata": {
        "id": "yHF89FPsLopu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F0ICCJ6H2Nt",
        "outputId": "a717d02c-d9d7-4b2b-df77-2e89e37a8ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Loss: 0.2921\n",
            "Epoch [40/100], Loss: 0.0685\n",
            "Epoch [60/100], Loss: 0.0047\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Prediction: 0.5044\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define a simple RNN model\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # Initialize hidden state if not provided\n",
        "        if hidden is None:\n",
        "            hidden = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "\n",
        "        # Forward pass through RNN\n",
        "        output, hidden = self.rnn(x, hidden)\n",
        "\n",
        "        # Apply output layer to the last time step\n",
        "        output = self.fc(output[:, -1, :])\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "# Example parameters\n",
        "input_size = 1    # Dimension of input feature\n",
        "hidden_size = 10  # Size of hidden layer\n",
        "output_size = 1   # Dimension of output\n",
        "batch_size = 5    # Number of sequences per batch\n",
        "seq_length = 20   # Length of each sequence\n",
        "\n",
        "# Create random data\n",
        "X = torch.randn(batch_size, seq_length, input_size)\n",
        "y = torch.randn(batch_size, output_size)\n",
        "\n",
        "# Initialize model\n",
        "model = SimpleRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model for a few epochs\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    output, hidden = model(X)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(output, y)\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Make a prediction\n",
        "with torch.no_grad():\n",
        "    test_input = torch.randn(1, seq_length, input_size)\n",
        "    prediction, _ = model(test_input)\n",
        "    print(f'Prediction: {prediction.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary Example"
      ],
      "metadata": {
        "id": "zhxVZW3DLD45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"I took the dog for a walk this morning\",\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Hello how are you doing today\"\n",
        "]\n",
        "\n",
        "vocab = set()\n",
        "for text in texts:\n",
        "    for word in text.lower().split():\n",
        "        vocab.add(word)\n",
        "vocab = list(vocab)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "print(f\"Vocabulary: {vocab}\")\n",
        "print(f\"Word to index mapping: {word_to_idx}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8cqGDfsLE9D",
        "outputId": "0be31ae0-80d9-4322-effd-d57297c30a26"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['walk', 'lazy', 'for', 'this', 'fox', 'are', 'a', 'took', 'morning', 'quick', 'you', 'over', 'the', 'brown', 'hello', 'how', 'i', 'doing', 'dog', 'jumps', 'today']\n",
            "Word to index mapping: {'walk': 0, 'lazy': 1, 'for': 2, 'this': 3, 'fox': 4, 'are': 5, 'a': 6, 'took': 7, 'morning': 8, 'quick': 9, 'you': 10, 'over': 11, 'the': 12, 'brown': 13, 'hello': 14, 'how': 15, 'i': 16, 'doing': 17, 'dog': 18, 'jumps': 19, 'today': 20}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization Example\n"
      ],
      "metadata": {
        "id": "OJMpQRDALN7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Simple word tokenization\n",
        "tokens = \"I love this movie!\".lower().split()\n",
        "# Result: ['i', 'love', 'this', 'movie!']\n",
        "\n",
        "# Better word tokenization (removing punctuation)\n",
        "import re\n",
        "tokens = re.findall(r'\\w+', \"I love this movie!\".lower())\n",
        "# Result: ['i', 'love', 'this', 'movie']"
      ],
      "metadata": {
        "id": "yvnGFL3nLNrP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word-to-Index Conversion Example"
      ],
      "metadata": {
        "id": "WJxLW1rGNaDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(text, word_to_idx, unknown_token_idx=0):\n",
        "  \"\"\"Converts a text string to a list of numerical indices.\n",
        "\n",
        "  Args:\n",
        "    text: The input text string.\n",
        "    word_to_idx: A dictionary mapping words to their indices.\n",
        "    unknown_token_idx: The index to use for words not found in the vocabulary.\n",
        "\n",
        "  Returns:\n",
        "    A list of indices representing the input text.\n",
        "  \"\"\"\n",
        "  return [word_to_idx.get(word.lower(), unknown_token_idx) for word in text.split()]\n",
        "\n",
        "# Example usage:\n",
        "texts = [\n",
        "    \"I took the dog for a walk this morning\",\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Hello how are you doing today\"\n",
        "]\n",
        "\n",
        "vocab = set()\n",
        "for text in texts:\n",
        "  for word in text.lower().split():\n",
        "    vocab.add(word)\n",
        "vocab = list(vocab)\n",
        "word_to_idx = {word: i + 1 for i, word in enumerate(vocab)}  # Start indices from 1\n",
        "\n",
        "for text in texts:\n",
        "  print(f\"Original text: {text}\")\n",
        "  indices = text_to_indices(text, word_to_idx)\n",
        "  print(f\"Indices: {indices}\")\n",
        "  print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4aUh5IhIkU3",
        "outputId": "31e89230-0502-4335-b6ac-ae5e369ee932"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: I took the dog for a walk this morning\n",
            "Indices: [17, 8, 13, 19, 3, 7, 1, 4, 9]\n",
            "---\n",
            "Original text: The quick brown fox jumps over the lazy dog\n",
            "Indices: [13, 10, 14, 5, 20, 12, 13, 2, 19]\n",
            "---\n",
            "Original text: Hello how are you doing today\n",
            "Indices: [15, 16, 6, 11, 18, 21]\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding Example"
      ],
      "metadata": {
        "id": "bbLTIX-NNigB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original sequences of different lengths\n",
        "sequences = [[1, 2, 3, 4], [5, 6], [7, 8, 9]]\n",
        "\n",
        "# Find maximum length\n",
        "max_length = max(len(seq) for seq in sequences)  # 4\n",
        "\n",
        "# Pad sequences to make them all the same length\n",
        "padded_sequences = [seq + [0] * (max_length - len(seq)) for seq in sequences]\n",
        "# Result: [[1, 2, 3, 4], [5, 6, 0, 0], [7, 8, 9, 0]]\n",
        ""
      ],
      "metadata": {
        "id": "rmEimsv3Is6x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings Example"
      ],
      "metadata": {
        "id": "vtu7DPrfNrkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example vocabulary (words)\n",
        "vocab = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \"hello\", \"how\", \"are\", \"you\", \"doing\", \"today\", \"i\", \"took\", \"for\", \"a\", \"walk\", \"this\", \"morning\"]\n",
        "\n",
        "# Example word-to-index mapping\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Example embedding layer\n",
        "embedding_dim = 5  # You can adjust this dimension\n",
        "embedding_layer = nn.Embedding(len(vocab), embedding_dim)\n",
        "\n",
        "# Example: Get the embedding for the word \"the\"\n",
        "word_idx = word_to_idx[\"the\"]\n",
        "word_idx_tensor = torch.tensor([word_idx])\n",
        "word_embedding = embedding_layer(word_idx_tensor)\n",
        "\n",
        "print(f\"Embedding for 'the': {word_embedding}\")\n",
        "\n",
        "# Example: Get the embeddings for a sequence of words\n",
        "sequence = [\"the\", \"quick\", \"brown\", \"fox\"]\n",
        "sequence_indices = [word_to_idx[word] for word in sequence]\n",
        "sequence_indices_tensor = torch.tensor(sequence_indices)\n",
        "sequence_embeddings = embedding_layer(sequence_indices_tensor)\n",
        "\n",
        "print(f\"Embeddings for the sequence: {sequence_embeddings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkp-nPwdIzYu",
        "outputId": "4f32b7e7-7a59-4a2b-8048-e5024ab045fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'the': tensor([[-0.7140,  0.8337, -0.9585,  0.4536,  1.2461]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "Embeddings for the sequence: tensor([[-0.7140,  0.8337, -0.9585,  0.4536,  1.2461],\n",
            "        [-2.3065, -1.2869,  0.2137, -2.1268, -0.1341],\n",
            "        [-1.0408, -0.7647, -0.0553,  1.2049, -0.9825],\n",
            "        [ 0.3040, -0.7172,  1.0554, -1.4534,  0.4652]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    }
  ]
}